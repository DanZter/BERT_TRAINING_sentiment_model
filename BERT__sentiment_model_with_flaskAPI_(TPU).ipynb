{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT__sentiment_model_with_flaskAPI (TPU)",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HsKL2CgSbQh88HAecIVSCnH1VTOgjgTZ",
      "authorship_tag": "ABX9TyN3XkT97XR/xrk8rMCp9x2a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanZter/BERT_TRAINING_sentiment_model/blob/master/BERT__sentiment_model_with_flaskAPI_(TPU).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhycw09090GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YmP46Y6-DBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93608e73-9a50-48af-8091-71b7677eef2d"
      },
      "source": [
        "VERSION = \"1.5\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4994  100  4994    0     0  71342      0 --:--:-- --:--:-- --:--:-- 71342\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-1.5 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (47.3.1)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.9)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 79.0 MiB/ 79.0 MiB]                                                \n",
            "Operation completed over 1 objects/79.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][106.6 MiB/106.6 MiB]                                                \n",
            "Operation completed over 1 objects/106.6 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+ab660ae\n",
            "Processing ./torch_xla-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.5\n",
            "Processing ./torchvision-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.5.0a0+ab660ae)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==1.5) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (383 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhqBMJ_g-DDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkiDlzEt-DGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "ce97fc55-2fe4-45f6-dbfd-7e92ed10d98e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0\n",
            "Running on TPU  ['10.16.170.202:8470']\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eAyegtu-DN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "f99d8590-e637-480a-ec57-4b9a7f91ce90"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0a0+ab660ae)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c092bdc943f721bcee35711859c647ab2431386dd39b3d442445667ef9b2ecac\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 3.2MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHk_Kx2G-DS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "from sklearn import model_selection\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# import logging\n",
        "# logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m9k63w1-Dae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 10\n",
        "# ACCUMULATION = 2\n",
        "BERT_PATH = \"/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased\"\n",
        "TRAINING_FILE = \"/content/drive/My Drive/Colab Notebooks/input/IMDB Dataset.csv\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
        "MODEL_PATH =\"/content/drive/My Drive/Colab Notebooks/models/bert_sentiment_model.bin\"\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhm3SbLA-DYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDataset:\n",
        "    def __init__(self, review, target):\n",
        "        self.review = review                     # the review text, a list\n",
        "        self.target = target                     # 0 or 1, a list\n",
        "        self.tokenizer = TOKENIZER\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):                           # returns the total length of data set\n",
        "        return len(self.review)\n",
        "\n",
        "    def __getitem__(self, item):                 # takes an 'item' and returns tokenizer of that item from data set\n",
        "        review = str(self.review[item])          # converts everything to str incase there exists numbers etc.\n",
        "        review = \" \".join(review.split())        # removes all unnecessary space\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(     # encode_plus can encode 2 strings at a time\n",
        "            review,\n",
        "            None,                                # since we use only 1 string at a time\n",
        "            add_special_tokens=True,             # adds cld, sep tokens\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"] # since only 1 string token_type_ids are same and unnecessary\n",
        "\n",
        "        padding_length = self.max_len - len(ids)  # for bert we pad on the right side\n",
        "        ids = ids + ([0] * padding_length)        # zero times the padding length\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[item], dtype=torch.float)\n",
        "        }\n",
        "    \"\"\" if we have 2 target outputs then set to torch.long,\n",
        "    depends on loss function also, from cross-entropy we should use torch.long\"\"\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7baJs1kv-DV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, target):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, target.view(-1, 1))\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        ids = d[\"ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        target = d[\"target\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)              # send to TPU device\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        target = target.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs, target)        # find loss\n",
        "        loss.backward()                         # backward propagation\n",
        "\n",
        "        xm.optimizer_step(optimizer)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        if bi % 10 == 0:\n",
        "            xm.master_print(f\"batch_index={bi}, loss={loss}\")\n",
        "\n",
        "        \"\"\" stop the optimizer only after a certain number of accumulation steps \"\"\"\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_target = []                         # final targets\n",
        "    fin_outputs = []                        # final outputs\n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            target = d[\"target\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)              # send to cuda device\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            target = target.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            # loss = loss_fn(outputs, targets)        # find loss, its bettwer to evaluate loss in eval fn\n",
        "\n",
        "            fin_target.extend(target.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_target"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00BBEZLJ-DQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "        \"\"\" 768: bert we use have 768 features | 1: binary classification\n",
        "        if we use 2, we need to change the loss function\"\"\"\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        \"\"\" We have 2 outputs from a BERT model\n",
        "         o1(last hidden state): is the sequence of hidden states. eg. if we have 512 tokens (MAX_LEN), \n",
        "         we have 512 vectors of size 768 for each batch. We can use out1 to max pooling or averge pooling\n",
        "         o2(pooler output from bert pooler layer): we get vector of size 768 for each sample in batch\"\"\"\n",
        "        bo = self.bert_drop(o2)                                 # drop-out\n",
        "        output = self.out(bo)                                   # linear-layer\n",
        "        return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQNzSGVi-DLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def run(index, flags):\n",
        "\n",
        "    flags['MAX_LEN'] = 256\n",
        "    flags['TRAIN_BATCH_SIZE'] = 32  # 9.47min for BS=8\n",
        "                                    # 5.54min for BS=32\n",
        "    flags['VALID_BATCH_SIZE'] = 16\n",
        "    flags['EPOCHS'] = 10\n",
        "    flags['seed'] = 1234  \n",
        "    torch.manual_seed(flags['seed'])\n",
        "\n",
        "    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
        "    dfx.sentiment = dfx.sentiment.apply(  # can use label encoding\n",
        "        lambda x: 1 if x == \"positive\" else 0  # can use map fn\n",
        "    )\n",
        "\n",
        "    df_train, df_valid = model_selection.train_test_split(\n",
        "        dfx,\n",
        "        test_size=0.1,\n",
        "        random_state=42,\n",
        "        stratify=dfx.sentiment.values  # when split both train and val have same positive to negative sample ratio\n",
        "    )\n",
        "\n",
        "    df_train = df_train.reset_index(drop=True)  # 0 to length of df_train\n",
        "    df_valid = df_valid.reset_index(drop=True)  # 0 to length of df_valid\n",
        "\n",
        "    train_dataset = BERTDataset(\n",
        "        review=df_train.review.values,\n",
        "        target=df_train.sentiment.values\n",
        "    )\n",
        "    train_sampler = torch.utils.data.DistributedSampler(\n",
        "       train_dataset,\n",
        "       num_replicas=xm.xrt_world_size(),\n",
        "       rank=xm.get_ordinal(),\n",
        "       shuffle=True \n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=flags['TRAIN_BATCH_SIZE'],\n",
        "        sampler=train_sampler\n",
        "    )\n",
        "####################################################\n",
        "    valid_dataset = BERTDataset(\n",
        "        review=df_valid.review.values,\n",
        "        target=df_valid.sentiment.values\n",
        "    )\n",
        "    valid_sampler = torch.utils.data.DistributedSampler(\n",
        "       valid_dataset,\n",
        "       num_replicas=xm.xrt_world_size(),\n",
        "       rank=xm.get_ordinal()\n",
        "    )\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=flags['VALID_BATCH_SIZE'],\n",
        "        sampler=valid_sampler\n",
        "    )\n",
        "\n",
        "    # device = \"cuda\"                   # cuda\n",
        "    device = xm.xla_device()            # tpu\n",
        "\n",
        "    model = BERTBaseUncased().to(device)\n",
        "    # lr = 3e-5 *xm.xrt_world_size()\n",
        "    param_optimizer = list(model.named_parameters())  # specify parameters to train\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    \"\"\" These parameters are adjustable, we should take a look at different layers and\n",
        "    the decay we want, how much learning rate etc.\"\"\"\n",
        "\n",
        "    num_train_steps = int(len(df_train) / flags['TRAIN_BATCH_SIZE'] / xm.xrt_world_size() * flags['EPOCHS'])\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    # model = nn.DataParallel(model)              # converting to multi gpu model\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in tqdm(range(flags['EPOCHS']), total=flags['EPOCHS']):\n",
        "\n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "        train_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler)\n",
        "\n",
        "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "        outputs, target = eval_fn(para_loader.per_device_loader(device), model, device)\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        accuracy = metrics.accuracy_score(target, outputs)\n",
        "        xm.master_print(f\"Accuracy score = {accuracy}\")\n",
        "        xm.save(model.state_dict(), MODEL_PATH)          # tpu  # saving the model only if it improves\n",
        "\n",
        "\n",
        "# flags = {}\n",
        "# xmp.spawn(run, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFq8lNiK-DJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "c6b903a4-b0d5-4ec3-8b69-3f255442b739"
      },
      "source": [
        "!pip install -U Flask\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: Flask in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask) (1.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdj7YKV_N9n3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "8e377ed2-d652-475d-8ba5-4dcc9b03603b"
      },
      "source": [
        "import flask\n",
        "from flask import Flask\n",
        "from flask import request, render_template\n",
        "\n",
        "import torch\n",
        "import flask\n",
        "import time\n",
        "from flask import Flask\n",
        "from flask import request\n",
        "\n",
        "import functools\n",
        "import torch.nn as nn\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)     # initialize flask app\n",
        "run_with_ngrok(app)\n",
        "MODEL = None\n",
        "DEVICE = xm.xla_device()\n",
        "PREDICTION_DICT = dict()\n",
        "# memory = joblib.Memory(\"../input/\", verbose=0)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "  return render_template(\"index.html\")\n",
        "\n",
        "def predict_from_cache(sentence):\n",
        "    if sentence in PREDICTION_DICT:\n",
        "        return PREDICTION_DICT[sentence]\n",
        "    else:\n",
        "        result = sentence_prediction(sentence)\n",
        "        PREDICTION_DICT[sentence] = result\n",
        "        return result\n",
        "\n",
        "\n",
        "@memory.cache\n",
        "def sentence_prediction(sentence):\n",
        "    tokenizer = TOKENIZER\n",
        "    max_len = MAX_LEN\n",
        "    review = str(sentence)\n",
        "    review = \" \".join(review.split())     # removes all unnecessary space\n",
        "\n",
        "    inputs = tokenizer.encode_plus(       # encode_plus can encode 2 strings at a time\n",
        "        review,\n",
        "        None,                             # since we use only 1 string at a time\n",
        "        add_special_tokens=True,          # adds cls, sep tokens\n",
        "        max_length=max_len,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    mask = inputs[\"attention_mask\"]\n",
        "    token_type_ids = inputs[\"token_type_ids\"]      # since only 1 string token_type_ids are same and unnecessary\n",
        "\n",
        "    padding_length = max_len - len(ids)            # for bert we pad on the right side\n",
        "    ids = ids + ([0] * padding_length)             # zero times the padding length\n",
        "    mask = mask + ([0] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)    # since dataloader always returns batches\n",
        "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
        "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    ids = ids.to(DEVICE, dtype=torch.long)          # send to tpu DEVICE\n",
        "    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
        "    mask = mask.to(DEVICE, dtype=torch.long)\n",
        "\n",
        "    outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
        "    return outputs[0][0]          # since outputs will be 2-Dimensional but there is only 1 value\n",
        "\n",
        "\n",
        "@app.route(\"/predict\", methods=['POST'])            # creating end point using flask\n",
        "def predict():                                      # predict function\n",
        "    sentence = [str(x) for x in request.form.values()]\n",
        "    sentence = sentence[0]\n",
        "    # sentence = str(request.form.values())\n",
        "    start_time = time.time()\n",
        "    positive_prediction = sentence_prediction(sentence)\n",
        "    negative_prediction = 1 - positive_prediction\n",
        "    response = {}\n",
        "    response[\"response\"] = {\n",
        "        \"positive\": str(positive_prediction),\n",
        "        \"negative\": str(negative_prediction),\n",
        "        \"sentence\": str(sentence),\n",
        "        \"time_taken\": str(time.time() - start_time),\n",
        "    }\n",
        "    return render_template(\"index.html\", sentence = r\"Your Sentence = '{}'\".format(sentence),\n",
        "    prediction_text=\"Sentiment of the sentence is :\", positive=  \"{}% POSITIVE\".format(round(positive_prediction*100,2)),\n",
        "    negative = \"{}% NEGATIVE\".format(round(negative_prediction*100,2)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL = BERTBaseUncased()\n",
        "    MODEL.load_state_dict(torch.load(MODEL_PATH))\n",
        "    MODEL.to(DEVICE)\n",
        "    MODEL.eval()\n",
        "    app.run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://5a0ea38a2576.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Jul/2020 01:39:13] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Jul/2020 01:39:13] \"\u001b[37mGET /static/css/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Jul/2020 01:39:14] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Jul/2020 01:39:16] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TaHk87S-Zz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# app = Flask(__name__)     # initialize flask app\n",
        "# run_with_ngrok(app)       # ngrok for colab VM\n",
        "\n",
        "# MODEL = None\n",
        "# DEVICE = xm.xla_device()\n",
        "# PREDICTION_DICT = dict()\n",
        "# memory = joblib.Memory(\"../input/\", verbose=0)\n",
        "\n",
        "\n",
        "# def predict_from_cache(sentence):\n",
        "#     if sentence in PREDICTION_DICT:\n",
        "#         return PREDICTION_DICT[sentence]\n",
        "#     else:\n",
        "#         result = sentence_prediction(sentence)\n",
        "#         PREDICTION_DICT[sentence] = result\n",
        "#         return result\n",
        "\n",
        "\n",
        "# @memory.cache\n",
        "# def sentence_prediction(sentence):\n",
        "#     tokenizer = TOKENIZER\n",
        "#     max_len = MAX_LEN\n",
        "#     review = str(sentence)\n",
        "#     review = \" \".join(review.split())     # removes all unnecessary space\n",
        "\n",
        "#     inputs = tokenizer.encode_plus(       # encode_plus can encode 2 strings at a time\n",
        "#         review, \n",
        "#         None,                             # since we use only 1 string at a time\n",
        "#         add_special_tokens=True,          # adds cls, sep tokens\n",
        "#         max_length=max_len\n",
        "#     )\n",
        "\n",
        "#     ids = inputs[\"input_ids\"]\n",
        "#     mask = inputs[\"attention_mask\"]\n",
        "#     token_type_ids = inputs[\"token_type_ids\"]      # since only 1 string token_type_ids are same and unnecessary\n",
        "\n",
        "#     padding_length = max_len - len(ids)            # for bert we pad on the right side\n",
        "#     ids = ids + ([0] * padding_length)             # zero times the padding length\n",
        "#     mask = mask + ([0] * padding_length)\n",
        "#     token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "#     ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)    # since dataloader always returns batches\n",
        "#     mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
        "#     token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "#     ids = ids.to(DEVICE, dtype=torch.long)          # send to tpu DEVICE\n",
        "#     token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
        "#     mask = mask.to(DEVICE, dtype=torch.long)\n",
        "\n",
        "#     outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "#     outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
        "#     return outputs[0][0]          # since outputs will be 2-Dimensional but there is only 1 value\n",
        "\n",
        "\n",
        "# @app.route(\"/predict\")            # creating end point using flask\n",
        "# def predict():                    # predict function\n",
        "#     sentence = request.args.get(\"sentence\")\n",
        "#     start_time = time.time()\n",
        "#     positive_prediction = sentence_prediction(sentence)\n",
        "#     negative_prediction = 1 - positive_prediction\n",
        "#     response = {}\n",
        "#     response[\"response\"] = {\n",
        "#         \"positive\": str(positive_prediction),\n",
        "#         \"negative\": str(negative_prediction),\n",
        "#         \"sentence\": str(sentence),\n",
        "#         \"time_taken\": str(time.time() - start_time),\n",
        "#     }\n",
        "#     return flask.jsonify(response)\n",
        "    \n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     MODEL = BERTBaseUncased()\n",
        "#     # MODEL = nn.DataParallel(MODEL)      # for multicore parallel training\n",
        "#     MODEL.load_state_dict(torch.load(MODEL_PATH))\n",
        "#     MODEL.to(DEVICE)\n",
        "#     MODEL.eval()\n",
        "#     app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}