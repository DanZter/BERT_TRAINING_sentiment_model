{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TRAINING_sentiment_model (TPU)",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pCNGn1E7zXV55140StRU_QvXi0E__24c",
      "authorship_tag": "ABX9TyMgHwSpg1XU5LszY5K94Mle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanZter/BERT_TRAINING_sentiment_model/blob/master/BERT_TRAINING_sentiment_model_(TPU).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uWOXQcoa8c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztDjHnJvbIK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "440d05e1-d032-4783-d07f-c3fe0e5c34e0"
      },
      "source": [
        "VERSION = \"1.5\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4994  100  4994    0     0  56750      0 --:--:-- --:--:-- --:--:-- 57402\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-1.5 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (47.3.1)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 79.0 MiB/ 79.0 MiB]                                                \n",
            "Operation completed over 1 objects/79.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][106.6 MiB/106.6 MiB]                                                \n",
            "Operation completed over 1 objects/106.6 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+ab660ae\n",
            "Processing ./torch_xla-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.5\n",
            "Processing ./torchvision-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.5.0a0+ab660ae)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==1.5) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (365 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTYY4KuGbIOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41T3XEiobIRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "2573b8fa-1f69-4d1b-8bab-d06a8660422d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0\n",
            "Running on TPU  ['10.96.254.26:8470']\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjXRODeRbITz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "44c8a135-449e-4e80-c016-e3d56a1ce9ab"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0a0+ab660ae)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=81ed45c893b671ec1800b64476274f8b32a677615c3809215146968159a84c56\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 3.2MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeaVY9aybIYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "from sklearn import model_selection\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# import logging\n",
        "# logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxMtdLxbIWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "# TRAIN_BATCH_SIZE = 8\n",
        "# VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 10\n",
        "# ACCUMULATION = 2\n",
        "BERT_PATH = \"/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased\"\n",
        "TRAINING_FILE = \"/content/drive/My Drive/Colab Notebooks/input/IMDB Dataset.csv\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
        "MODEL_PATH =\"/content/drive/My Drive/Colab Notebooks/models/bert_sentiment_model.bin\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x5XNys3bTin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDataset:\n",
        "    def __init__(self, review, target):\n",
        "        self.review = review                     # the review text, a list\n",
        "        self.target = target                     # 0 or 1, a list\n",
        "        self.tokenizer = TOKENIZER\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):                           # returns the total length of data set\n",
        "        return len(self.review)\n",
        "\n",
        "    def __getitem__(self, item):                 # takes an 'item' and returns tokenizer of that item from data set\n",
        "        review = str(self.review[item])          # converts everything to str incase there exists numbers etc.\n",
        "        review = \" \".join(review.split())        # removes all unnecessary space\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(     # encode_plus can encode 2 strings at a time\n",
        "            review,\n",
        "            None,                                # since we use only 1 string at a time\n",
        "            add_special_tokens=True,             # adds cld, sep tokens\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"] # since only 1 string token_type_ids are same and unnecessary\n",
        "\n",
        "        padding_length = self.max_len - len(ids)  # for bert we pad on the right side\n",
        "        ids = ids + ([0] * padding_length)        # zero times the padding length\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[item], dtype=torch.float)\n",
        "        }\n",
        "    \"\"\" if we have 2 target outputs then set to torch.long,\n",
        "    depends on loss function also, from cross-entropy we should use torch.long\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKK9-A2VbTlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, target):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, target.view(-1, 1))\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        ids = d[\"ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        target = d[\"target\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)              # send to TPU device\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        target = target.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs, target)        # find loss\n",
        "        loss.backward()                         # backward propagation\n",
        "\n",
        "        xm.optimizer_step(optimizer)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        if bi % 10 == 0:\n",
        "            xm.master_print(f\"batch_index={bi}, loss={loss}\")\n",
        "\n",
        "        \"\"\" stop the optimizer only after a certain number of accumulation steps \"\"\"\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_target = []                         # final targets\n",
        "    fin_outputs = []                        # final outputs\n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            target = d[\"target\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)              # send to cuda device\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            target = target.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            # loss = loss_fn(outputs, targets)        # find loss, its bettwer to evaluate loss in eval fn\n",
        "\n",
        "            fin_target.extend(target.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99T_bju7bTv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "        \"\"\" 768: bert we use have 768 features | 1: binary classification\n",
        "        if we use 2, we need to change the loss function\"\"\"\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        \"\"\" We have 2 outputs from a BERT model\n",
        "         o1(last hidden state): is the sequence of hidden states. eg. if we have 512 tokens (MAX_LEN), \n",
        "         we have 512 vectors of size 768 for each batch. We can use out1 to max pooling or averge pooling\n",
        "         o2(pooler output from bert pooler layer): we get vector of size 768 for each sample in batch\"\"\"\n",
        "        bo = self.bert_drop(o2)                                 # drop-out\n",
        "        output = self.out(bo)                                   # linear-layer\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2GXy15TbTsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "626611fd-48a9-4c3f-9e4c-fab917699306"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def run(index, flags):\n",
        "\n",
        "    flags['MAX_LEN'] = 256\n",
        "    flags['TRAIN_BATCH_SIZE'] = 32  # 9.47min for BS=8\n",
        "                                    # 5.54min for BS=32\n",
        "    flags['VALID_BATCH_SIZE'] = 16\n",
        "    flags['EPOCHS'] = 10\n",
        "    flags['seed'] = 1234  \n",
        "    torch.manual_seed(flags['seed'])\n",
        "\n",
        "    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
        "    dfx.sentiment = dfx.sentiment.apply(  # can use label encoding\n",
        "        lambda x: 1 if x == \"positive\" else 0  # can use map fn\n",
        "    )\n",
        "\n",
        "    df_train, df_valid = model_selection.train_test_split(\n",
        "        dfx,\n",
        "        test_size=0.1,\n",
        "        random_state=42,\n",
        "        stratify=dfx.sentiment.values  # when split both train and val have same positive to negative sample ratio\n",
        "    )\n",
        "\n",
        "    df_train = df_train.reset_index(drop=True)  # 0 to length of df_train\n",
        "    df_valid = df_valid.reset_index(drop=True)  # 0 to length of df_valid\n",
        "\n",
        "    train_dataset = BERTDataset(\n",
        "        review=df_train.review.values,\n",
        "        target=df_train.sentiment.values\n",
        "    )\n",
        "    train_sampler = torch.utils.data.DistributedSampler(\n",
        "       train_dataset,\n",
        "       num_replicas=xm.xrt_world_size(),\n",
        "       rank=xm.get_ordinal(),\n",
        "       shuffle=True \n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=flags['TRAIN_BATCH_SIZE'],\n",
        "        sampler=train_sampler\n",
        "    )\n",
        "####################################################\n",
        "    valid_dataset = BERTDataset(\n",
        "        review=df_valid.review.values,\n",
        "        target=df_valid.sentiment.values\n",
        "    )\n",
        "    valid_sampler = torch.utils.data.DistributedSampler(\n",
        "       valid_dataset,\n",
        "       num_replicas=xm.xrt_world_size(),\n",
        "       rank=xm.get_ordinal()\n",
        "    )\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=flags['VALID_BATCH_SIZE'],\n",
        "        sampler=valid_sampler\n",
        "    )\n",
        "\n",
        "    # device = \"cuda\"                   # cuda\n",
        "    device = xm.xla_device()            # tpu\n",
        "\n",
        "    model = BERTBaseUncased().to(device)\n",
        "    # lr = 3e-5 *xm.xrt_world_size()\n",
        "    param_optimizer = list(model.named_parameters())  # specify parameters to train\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    \"\"\" These parameters are adjustable, we should take a look at different layers and\n",
        "    the decay we want, how much learning rate etc.\"\"\"\n",
        "\n",
        "    num_train_steps = int(len(df_train) / flags['TRAIN_BATCH_SIZE'] / xm.xrt_world_size() * flags['EPOCHS'])\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    # model = nn.DataParallel(model)              # converting to multi gpu model\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in tqdm(range(flags['EPOCHS']), total=flags['EPOCHS']):\n",
        "\n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "        train_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler)\n",
        "\n",
        "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "        outputs, target = eval_fn(para_loader.per_device_loader(device), model, device)\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        accuracy = metrics.accuracy_score(target, outputs)\n",
        "        xm.master_print(f\"Accuracy score = {accuracy}\")\n",
        "        xm.save(model.state_dict(), MODEL_PATH)          # tpu  # saving the model only if it improves\n",
        "\n",
        "\n",
        "flags = {}\n",
        "xmp.spawn(run, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.712925910949707\n",
            "batch_index=10, loss=0.572005033493042\n",
            "batch_index=20, loss=0.41632986068725586\n",
            "batch_index=30, loss=0.3658309280872345\n",
            "batch_index=40, loss=0.3177637457847595\n",
            "batch_index=50, loss=0.3634778559207916\n",
            "batch_index=60, loss=0.2803698182106018\n",
            "batch_index=70, loss=0.10925929993391037\n",
            "batch_index=80, loss=0.3809845447540283\n",
            "batch_index=90, loss=0.2588035464286804\n",
            "batch_index=100, loss=0.2224305421113968\n",
            "batch_index=110, loss=0.14605173468589783\n",
            "batch_index=120, loss=0.2638278007507324\n",
            "batch_index=130, loss=0.1329445242881775\n",
            "batch_index=140, loss=0.14641815423965454\n",
            "batch_index=150, loss=0.3915320038795471\n",
            "batch_index=160, loss=0.21254122257232666\n",
            "batch_index=170, loss=0.25655731558799744\n",
            "Accuracy score = 0.9088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [04:55<44:15, 295.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.140375554561615\n",
            "batch_index=10, loss=0.0999813824892044\n",
            "batch_index=20, loss=0.1271202713251114\n",
            "batch_index=30, loss=0.19300946593284607\n",
            "batch_index=40, loss=0.08767985552549362\n",
            "batch_index=50, loss=0.3392219841480255\n",
            "batch_index=60, loss=0.08976869285106659\n",
            "batch_index=70, loss=0.1573590338230133\n",
            "batch_index=80, loss=0.16194798052310944\n",
            "batch_index=90, loss=0.0737970694899559\n",
            "batch_index=100, loss=0.09647437930107117\n",
            "batch_index=110, loss=0.024589549750089645\n",
            "batch_index=120, loss=0.07751443982124329\n",
            "batch_index=130, loss=0.03608041629195213\n",
            "batch_index=140, loss=0.02505209855735302\n",
            "batch_index=150, loss=0.13865438103675842\n",
            "batch_index=160, loss=0.09108159691095352\n",
            "batch_index=170, loss=0.0880570262670517\n",
            "Accuracy score = 0.9088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [09:52<39:26, 295.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.04677693173289299\n",
            "batch_index=10, loss=0.022484421730041504\n",
            "batch_index=20, loss=0.032779496163129807\n",
            "batch_index=30, loss=0.19956190884113312\n",
            "batch_index=40, loss=0.07816101610660553\n",
            "batch_index=50, loss=0.2166823446750641\n",
            "batch_index=60, loss=0.021408211439847946\n",
            "batch_index=70, loss=0.01058275904506445\n",
            "batch_index=80, loss=0.11363545060157776\n",
            "batch_index=90, loss=0.04415140300989151\n",
            "batch_index=100, loss=0.07903831452131271\n",
            "batch_index=110, loss=0.024119393900036812\n",
            "batch_index=120, loss=0.031549327075481415\n",
            "batch_index=130, loss=0.00860715750604868\n",
            "batch_index=140, loss=0.006900558713823557\n",
            "batch_index=150, loss=0.005322813522070646\n",
            "batch_index=160, loss=0.10207431018352509\n",
            "batch_index=170, loss=0.04121142625808716\n",
            "Accuracy score = 0.9088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [14:48<34:31, 295.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.006958787329494953\n",
            "batch_index=10, loss=0.040407419204711914\n",
            "batch_index=20, loss=0.004069489426910877\n",
            "batch_index=30, loss=0.19955193996429443\n",
            "batch_index=40, loss=0.009213624522089958\n",
            "batch_index=50, loss=0.21830454468727112\n",
            "batch_index=60, loss=0.004754527006298304\n",
            "batch_index=70, loss=0.005360993556678295\n",
            "batch_index=80, loss=0.004693830851465464\n",
            "batch_index=90, loss=0.005153260193765163\n",
            "batch_index=100, loss=0.011201776564121246\n",
            "batch_index=110, loss=0.0020105289295315742\n",
            "batch_index=120, loss=0.004457641392946243\n",
            "batch_index=130, loss=0.005420117173343897\n",
            "batch_index=140, loss=0.0029086994472891092\n",
            "batch_index=150, loss=0.06882978975772858\n",
            "batch_index=160, loss=0.0035133955534547567\n",
            "batch_index=170, loss=0.00819825567305088\n",
            "Accuracy score = 0.9136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [19:45<29:37, 296.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.0113910511136055\n",
            "batch_index=10, loss=0.00468366127461195\n",
            "batch_index=20, loss=0.002625690307468176\n",
            "batch_index=30, loss=0.1776151806116104\n",
            "batch_index=40, loss=0.005448945332318544\n",
            "batch_index=50, loss=0.1945873200893402\n",
            "batch_index=60, loss=0.002370615489780903\n",
            "batch_index=70, loss=0.002087697619572282\n",
            "batch_index=80, loss=0.0032951654866337776\n",
            "batch_index=90, loss=0.0032719289883971214\n",
            "batch_index=100, loss=0.006449071224778891\n",
            "batch_index=110, loss=0.0014277392765507102\n",
            "batch_index=120, loss=0.003715003142133355\n",
            "batch_index=130, loss=0.002448472660034895\n",
            "batch_index=140, loss=0.016254251822829247\n",
            "batch_index=150, loss=0.00309423147700727\n",
            "batch_index=160, loss=0.0038798206951469183\n",
            "batch_index=170, loss=0.0023498539812862873\n",
            "Accuracy score = 0.9136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [24:41<24:40, 296.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.0015783410053700209\n",
            "batch_index=10, loss=0.0017739732284098864\n",
            "batch_index=20, loss=0.0015770461177453399\n",
            "batch_index=30, loss=0.11720628291368484\n",
            "batch_index=40, loss=0.0022287112660706043\n",
            "batch_index=50, loss=0.16918465495109558\n",
            "batch_index=60, loss=0.001902380376122892\n",
            "batch_index=70, loss=0.0015445915050804615\n",
            "batch_index=80, loss=0.0032652404624968767\n",
            "batch_index=90, loss=0.002165313810110092\n",
            "batch_index=100, loss=0.003895846661180258\n",
            "batch_index=110, loss=0.0011393469758331776\n",
            "batch_index=120, loss=0.0020371803548187017\n",
            "batch_index=130, loss=0.0015736910281702876\n",
            "batch_index=140, loss=0.0014171915827319026\n",
            "batch_index=150, loss=0.0015174549771472812\n",
            "batch_index=160, loss=0.001783657819032669\n",
            "batch_index=170, loss=0.0015186441596597433\n",
            "Accuracy score = 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [29:41<19:48, 297.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.0010004714131355286\n",
            "batch_index=10, loss=0.0010493409354239702\n",
            "batch_index=20, loss=0.0012820676201954484\n",
            "batch_index=30, loss=0.05666448548436165\n",
            "batch_index=40, loss=0.0017914227209985256\n",
            "batch_index=50, loss=0.14270800352096558\n",
            "batch_index=60, loss=0.0010516492184251547\n",
            "batch_index=70, loss=0.0010252107167616487\n",
            "batch_index=80, loss=0.0017762468196451664\n",
            "batch_index=90, loss=0.0013565690023824573\n",
            "batch_index=100, loss=0.0020585397724062204\n",
            "batch_index=110, loss=0.0008195889531634748\n",
            "batch_index=120, loss=0.0011738167377188802\n",
            "batch_index=130, loss=0.0010794235859066248\n",
            "batch_index=140, loss=0.0010797327850013971\n",
            "batch_index=150, loss=0.001268241903744638\n",
            "batch_index=160, loss=0.0015641690697520971\n",
            "batch_index=170, loss=0.001306099002249539\n",
            "Accuracy score = 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [34:38<14:51, 297.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.0008136151591315866\n",
            "batch_index=10, loss=0.0008382925298064947\n",
            "batch_index=20, loss=0.0009448332129977643\n",
            "batch_index=30, loss=0.013222131878137589\n",
            "batch_index=40, loss=0.001336161745712161\n",
            "batch_index=50, loss=0.11692651361227036\n",
            "batch_index=60, loss=0.000845984264742583\n",
            "batch_index=70, loss=0.0009438914712518454\n",
            "batch_index=80, loss=0.0015039315912872553\n",
            "batch_index=90, loss=0.001390160177834332\n",
            "batch_index=100, loss=0.001552528003230691\n",
            "batch_index=110, loss=0.0007501765503548086\n",
            "batch_index=120, loss=0.001045164535753429\n",
            "batch_index=130, loss=0.0008868241566233337\n",
            "batch_index=140, loss=0.0008830903680063784\n",
            "batch_index=150, loss=0.0010794780682772398\n",
            "batch_index=160, loss=0.0016825892962515354\n",
            "batch_index=170, loss=0.001226810971274972\n",
            "Accuracy score = 0.9136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [39:33<09:53, 296.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.000792522681877017\n",
            "batch_index=10, loss=0.0007040736963972449\n",
            "batch_index=20, loss=0.0007748660864308476\n",
            "batch_index=30, loss=0.0008955808007158339\n",
            "batch_index=40, loss=0.0009666133555583656\n",
            "batch_index=50, loss=0.10031946003437042\n",
            "batch_index=60, loss=0.0006993578281253576\n",
            "batch_index=70, loss=0.000798467721324414\n",
            "batch_index=80, loss=0.0012200031196698546\n",
            "batch_index=90, loss=0.001264916267246008\n",
            "batch_index=100, loss=0.001317988382652402\n",
            "batch_index=110, loss=0.000631515693385154\n",
            "batch_index=120, loss=0.0009193432633765042\n",
            "batch_index=130, loss=0.0007406898657791317\n",
            "batch_index=140, loss=0.0007438664324581623\n",
            "batch_index=150, loss=0.0009169376571662724\n",
            "batch_index=160, loss=0.0014492084737867117\n",
            "batch_index=170, loss=0.0011345382081344724\n",
            "Accuracy score = 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [44:32<04:57, 297.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_index=0, loss=0.0006689302390441298\n",
            "batch_index=10, loss=0.0006125119398348033\n",
            "batch_index=20, loss=0.0006922961329109967\n",
            "batch_index=30, loss=0.0007985163829289377\n",
            "batch_index=40, loss=0.00079924869351089\n",
            "batch_index=50, loss=0.09079866856336594\n",
            "batch_index=60, loss=0.0006292837206274271\n",
            "batch_index=70, loss=0.0007417433080263436\n",
            "batch_index=80, loss=0.0010458879405632615\n",
            "batch_index=90, loss=0.001201584585942328\n",
            "batch_index=100, loss=0.0011632515816017985\n",
            "batch_index=110, loss=0.0005571283982135355\n",
            "batch_index=120, loss=0.0008400609949603677\n",
            "batch_index=130, loss=0.0006612197612412274\n",
            "batch_index=140, loss=0.0006405180902220309\n",
            "batch_index=150, loss=0.0007852810667827725\n",
            "batch_index=160, loss=0.0012958534061908722\n",
            "batch_index=170, loss=0.0010764047037810087\n",
            "Accuracy score = 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [50:18<00:00, 310.72s/it]\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 10/10 [50:19<00:00, 301.91s/it]\n",
            "100%|██████████| 10/10 [50:13<00:00, 301.36s/it]\n",
            "\n",
            "100%|██████████| 10/10 [50:13<00:00, 301.39s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}