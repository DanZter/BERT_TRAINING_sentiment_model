{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TRAINING_sentiment_model (GPU)",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13h5spHRwOa3V-AvWZetCNTrPHBTejQQx",
      "authorship_tag": "ABX9TyOJDfdtYlyMkEF9LWeBd4Ww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanZter/BERT_TRAINING_sentiment_model/blob/master/BERT_TRAINING_sentiment_model_(GPU).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe3iQ2AL4nDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "6ab28574-8b1e-4885-be35-31458de40a20"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1a576edd1e6b1bdce1e51e58fc2b20b50f56cd8d2f49dc42ec37ec23d2f31731\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 6.0MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLJqwNcSwtLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "from sklearn import model_selection\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# import logging\n",
        "# logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WR98cTK5GPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 10\n",
        "# ACCUMULATION = 2\n",
        "BERT_PATH = \"/content/drive/My Drive/Colab Notebooks/input/bert_base_uncased\"\n",
        "TRAINING_FILE = \"/content/drive/My Drive/Colab Notebooks/input/IMDB Dataset.csv\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
        "MODEL_PATH =\"/content/drive/My Drive/Colab Notebooks/models/bert_sentiment_model.bin\"\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSj84ggQ5LqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDataset:\n",
        "    def __init__(self, review, target):\n",
        "        self.review = review                     # the review text, a list\n",
        "        self.target = target                     # 0 or 1, a list\n",
        "        self.tokenizer = TOKENIZER\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):                           # returns the total length of data set\n",
        "        return len(self.review)\n",
        "\n",
        "    def __getitem__(self, item):                 # takes an 'item' and returns tokenizer of that item from data set\n",
        "        review = str(self.review[item])          # converts everything to str incase there exists numbers etc.\n",
        "        review = \" \".join(review.split())        # removes all unnecessary space\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(     # encode_plus can encode 2 strings at a time\n",
        "            review,\n",
        "            None,                                # since we use only 1 string at a time\n",
        "            add_special_tokens=True,             # adds cld, sep tokens\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"] # since only 1 string token_type_ids are same and unnecessary\n",
        "\n",
        "        padding_length = self.max_len - len(ids)  # for bert we pad on the right side\n",
        "        ids = ids + ([0] * padding_length)        # zero times the padding length\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.target[item], dtype=torch.float)\n",
        "        }\n",
        "    \"\"\" if we have 2 target outputs then set to torch.long,\n",
        "    depends on loss function also, from cross-entropy we should use torch.long\"\"\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkbneWSG5Oma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, target):\n",
        "    return nn.BCEWithLogitsLoss()(outputs, target.view(-1, 1))\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    for bi, d in enumerate(data_loader):\n",
        "        ids = d[\"ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        target = d[\"target\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)              # send to cuda device\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        target = target.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs, target)        # find loss\n",
        "        loss.backward()                         # backward propagation\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        \"\"\" stop the optimizer only after a certain number of accumulation steps \"\"\"\n",
        "\n",
        "        # if (bi + 1) % accumulation_steps == 0:\n",
        "        #     optimizer.step()\n",
        "        #     scheduler.step\n",
        "        if bi % 10 == 0:\n",
        "            print(f\"batch_index={bi}, loss={loss}\")\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_target = []                         # final targets\n",
        "    fin_outputs = []                        # final outputs\n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            target = d[\"target\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)              # send to cuda device\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            target = target.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            # loss = loss_fn(outputs, targets)        # find loss, its bettwer to evaluate loss in eval fn\n",
        "\n",
        "            fin_target.extend(target.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_target"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oUJFIjK5RLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import torch.nn as nn\n",
        "\n",
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "        \"\"\" 768: bert we use have 768 features | 1: binary classification\n",
        "        if we use 2, we need to change the loss function\"\"\"\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        \"\"\" We have 2 outputs from a BERT model\n",
        "         o1(last hidden state): is the sequence of hidden states. eg. if we have 512 tokens (MAX_LEN), \n",
        "         we have 512 vectors of size 768 for each batch. We can use out1 to max pooling or averge pooling\n",
        "         o2(pooler output from bert pooler layer): we get vector of size 768 for each sample in batch\"\"\"\n",
        "        bo = self.bert_drop(o2)                                 # drop-out\n",
        "        output = self.out(bo)                                   # linear-layer\n",
        "        return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyhG7dH95Wic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdfeab86-b3aa-42f9-9fc1-e5228c889ba8"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import torch.nn as nn       # for multi-gpu\n",
        "\n",
        "# from model import BERTBaseUncased\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "def run():\n",
        "    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
        "    dfx.sentiment = dfx.sentiment.apply(  # can use label encoding\n",
        "        lambda x: 1 if x == \"positive\" else 0  # can use map fn\n",
        "    )\n",
        "\n",
        "    df_train, df_valid = model_selection.train_test_split(\n",
        "        dfx,\n",
        "        test_size=0.1,\n",
        "        random_state=42,\n",
        "        stratify=dfx.sentiment.values  # when split both train and val have same positive to negative sample ratio\n",
        "    )\n",
        "\n",
        "    df_train = df_train.reset_index(drop=True)  # 0 to length of df_train\n",
        "    df_valid = df_valid.reset_index(drop=True)  # 0 to length of df_valid\n",
        "\n",
        "    train_dataset = BERTDataset(\n",
        "        review=df_train.review.values,\n",
        "        target=df_train.sentiment.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        num_workers=4\n",
        "    )\n",
        "    valid_dataset = BERTDataset(\n",
        "        review=df_valid.review.values,\n",
        "        target=df_valid.sentiment.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\")  # using cuda\n",
        "    print(torch.cuda.is_available(), device)\n",
        "    model = BERTBaseUncased().to(device)  # calling from model.py\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())  # specify parameters to train\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    \"\"\" These parameters are adjustable, we should take a look at different layers and\n",
        "    the decay we want, how much learning rate etc.\"\"\"\n",
        "\n",
        "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    # model = nn.DataParallel(model)              # converting to multi gpu model\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n",
        "        print(\"X\"*20, \"EPOCH :\", epoch, \"X\"*100)\n",
        "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
        "        outputs, target = eval_fn(valid_data_loader, model, device)\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        accuracy = metrics.accuracy_score(target, outputs)\n",
        "        print(f\"Accuracy score = {accuracy}\")\n",
        "        if accuracy > best_accuracy:\n",
        "            # torch.save(model.state_dict(), MODEL_PATH)  # saving the model only if it improves\n",
        "            best_accuracy = accuracy\n",
        "\n",
        "\n",
        "run().to(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XXXXXXXXXXXXXXXXXXXX EPOCH : 0 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "batch_index=0, loss=0.630736231803894\n",
            "batch_index=10, loss=0.5436104536056519\n",
            "batch_index=20, loss=0.6880532503128052\n",
            "batch_index=30, loss=0.7165029048919678\n",
            "batch_index=40, loss=0.5221825242042542\n",
            "batch_index=50, loss=0.612755537033081\n",
            "batch_index=60, loss=0.4160301089286804\n",
            "batch_index=70, loss=0.5801146626472473\n",
            "batch_index=80, loss=0.10606885701417923\n",
            "batch_index=90, loss=0.5991536974906921\n",
            "batch_index=100, loss=0.29199567437171936\n",
            "batch_index=110, loss=0.6438760757446289\n",
            "batch_index=120, loss=0.4128738045692444\n",
            "batch_index=130, loss=0.42486417293548584\n",
            "batch_index=140, loss=0.13386133313179016\n",
            "batch_index=150, loss=0.13998660445213318\n",
            "batch_index=160, loss=0.09322542697191238\n",
            "batch_index=170, loss=0.30200475454330444\n",
            "batch_index=180, loss=0.5537563562393188\n",
            "batch_index=190, loss=0.23777258396148682\n",
            "batch_index=200, loss=0.6004219651222229\n",
            "batch_index=210, loss=0.2014530897140503\n",
            "batch_index=220, loss=0.16779303550720215\n",
            "batch_index=230, loss=0.5103975534439087\n",
            "batch_index=240, loss=0.18305723369121552\n",
            "batch_index=250, loss=0.7007221579551697\n",
            "batch_index=260, loss=0.6607738733291626\n",
            "batch_index=270, loss=0.10387666523456573\n",
            "batch_index=280, loss=0.14430636167526245\n",
            "batch_index=290, loss=0.06990926712751389\n",
            "batch_index=300, loss=0.043760746717453\n",
            "batch_index=310, loss=0.13023008406162262\n",
            "batch_index=320, loss=0.3348201811313629\n",
            "batch_index=330, loss=0.23276102542877197\n",
            "batch_index=340, loss=0.041276559233665466\n",
            "batch_index=350, loss=0.27594125270843506\n",
            "batch_index=360, loss=0.5577501058578491\n",
            "batch_index=370, loss=0.21449977159500122\n",
            "batch_index=380, loss=0.37687522172927856\n",
            "batch_index=390, loss=0.6945596933364868\n",
            "batch_index=400, loss=0.13681858777999878\n",
            "batch_index=410, loss=0.42839401960372925\n",
            "batch_index=420, loss=0.026168853044509888\n",
            "batch_index=430, loss=0.3911793828010559\n",
            "batch_index=440, loss=0.4336705505847931\n",
            "batch_index=450, loss=0.21608416736125946\n",
            "batch_index=460, loss=0.16359035670757294\n",
            "batch_index=470, loss=0.4766918420791626\n",
            "batch_index=480, loss=0.07209338247776031\n",
            "batch_index=490, loss=0.13518081605434418\n",
            "batch_index=500, loss=0.9239128828048706\n",
            "batch_index=510, loss=0.2727256119251251\n",
            "batch_index=520, loss=0.1023159921169281\n",
            "batch_index=530, loss=0.45004919171333313\n",
            "batch_index=540, loss=0.2276114523410797\n",
            "batch_index=550, loss=0.1377553641796112\n",
            "batch_index=560, loss=0.3881152868270874\n",
            "batch_index=570, loss=0.564946711063385\n",
            "batch_index=580, loss=0.2939184904098511\n",
            "batch_index=590, loss=0.2092844843864441\n",
            "batch_index=600, loss=0.027852142229676247\n",
            "batch_index=610, loss=0.6904155015945435\n",
            "batch_index=620, loss=0.15773576498031616\n",
            "batch_index=630, loss=0.19855791330337524\n",
            "batch_index=640, loss=0.04408704861998558\n",
            "batch_index=650, loss=0.6529468297958374\n",
            "batch_index=660, loss=0.4288909435272217\n",
            "batch_index=670, loss=0.48774778842926025\n",
            "batch_index=680, loss=0.3189636468887329\n",
            "batch_index=690, loss=0.0930725559592247\n",
            "batch_index=700, loss=0.20061668753623962\n",
            "batch_index=710, loss=0.1373775750398636\n",
            "batch_index=720, loss=0.2575756013393402\n",
            "batch_index=730, loss=1.0509170293807983\n",
            "batch_index=740, loss=0.22560279071331024\n",
            "batch_index=750, loss=0.1073690876364708\n",
            "batch_index=760, loss=0.09847834706306458\n",
            "batch_index=770, loss=0.09741242974996567\n",
            "batch_index=780, loss=0.22688613831996918\n",
            "batch_index=790, loss=0.904482901096344\n",
            "batch_index=800, loss=0.020826103165745735\n",
            "batch_index=810, loss=0.4813164472579956\n",
            "batch_index=820, loss=0.4908837676048279\n",
            "batch_index=830, loss=0.6078437566757202\n",
            "batch_index=840, loss=0.4118131995201111\n",
            "batch_index=850, loss=0.10496362298727036\n",
            "batch_index=860, loss=0.615924596786499\n",
            "batch_index=870, loss=0.37289682030677795\n",
            "batch_index=880, loss=0.2047233134508133\n",
            "batch_index=890, loss=0.06899573653936386\n",
            "batch_index=900, loss=0.568630039691925\n",
            "batch_index=910, loss=0.14320455491542816\n",
            "batch_index=920, loss=0.10373100638389587\n",
            "batch_index=930, loss=0.5002442598342896\n",
            "batch_index=940, loss=0.3240695595741272\n",
            "batch_index=950, loss=0.11438553035259247\n",
            "batch_index=960, loss=0.4078540802001953\n",
            "batch_index=970, loss=0.059912580996751785\n",
            "batch_index=980, loss=0.25440508127212524\n",
            "batch_index=990, loss=0.21576789021492004\n",
            "batch_index=1000, loss=0.02023809775710106\n",
            "batch_index=1010, loss=0.22103995084762573\n",
            "batch_index=1020, loss=0.4126990735530853\n",
            "batch_index=1030, loss=0.3631386160850525\n",
            "batch_index=1040, loss=0.08581846952438354\n",
            "batch_index=1050, loss=0.16597044467926025\n",
            "batch_index=1060, loss=0.5604265928268433\n",
            "batch_index=1070, loss=0.14158964157104492\n",
            "batch_index=1080, loss=0.15213435888290405\n",
            "batch_index=1090, loss=0.3316960334777832\n",
            "batch_index=1100, loss=0.12822328507900238\n",
            "batch_index=1110, loss=0.5552033185958862\n",
            "batch_index=1120, loss=0.20640988647937775\n",
            "batch_index=1130, loss=0.35902562737464905\n",
            "batch_index=1140, loss=0.07514549046754837\n",
            "batch_index=1150, loss=0.03676135838031769\n",
            "batch_index=1160, loss=0.044047705829143524\n",
            "batch_index=1170, loss=0.03472347557544708\n",
            "batch_index=1180, loss=0.27249354124069214\n",
            "batch_index=1190, loss=0.2523888647556305\n",
            "batch_index=1200, loss=0.09420205652713776\n",
            "batch_index=1210, loss=0.4824329614639282\n",
            "batch_index=1220, loss=0.06309908628463745\n",
            "batch_index=1230, loss=0.2098952680826187\n",
            "batch_index=1240, loss=0.27230602502822876\n",
            "batch_index=1250, loss=0.10057403147220612\n",
            "batch_index=1260, loss=0.0383153073489666\n",
            "batch_index=1270, loss=0.2089117467403412\n",
            "batch_index=1280, loss=0.14390993118286133\n",
            "batch_index=1290, loss=0.1254289597272873\n",
            "batch_index=1300, loss=0.042611002922058105\n",
            "batch_index=1310, loss=0.1693914830684662\n",
            "batch_index=1320, loss=0.5864381790161133\n",
            "batch_index=1330, loss=0.6307076811790466\n",
            "batch_index=1340, loss=0.1699344962835312\n",
            "batch_index=1350, loss=0.2501113712787628\n",
            "batch_index=1360, loss=0.9530406594276428\n",
            "batch_index=1370, loss=0.27609458565711975\n",
            "batch_index=1380, loss=0.13260354101657867\n",
            "batch_index=1390, loss=0.13077320158481598\n",
            "batch_index=1400, loss=0.11524497717618942\n",
            "batch_index=1410, loss=0.8183227777481079\n",
            "batch_index=1420, loss=0.12733983993530273\n",
            "batch_index=1430, loss=0.3128945231437683\n",
            "batch_index=1440, loss=0.18272042274475098\n",
            "batch_index=1450, loss=0.38676485419273376\n",
            "batch_index=1460, loss=0.22381740808486938\n",
            "batch_index=1470, loss=0.29617613554000854\n",
            "batch_index=1480, loss=0.431801974773407\n",
            "batch_index=1490, loss=0.10668972879648209\n",
            "batch_index=1500, loss=0.1475924700498581\n",
            "batch_index=1510, loss=0.8413491249084473\n",
            "batch_index=1520, loss=0.23161077499389648\n",
            "batch_index=1530, loss=0.2693272829055786\n",
            "batch_index=1540, loss=0.034410297870635986\n",
            "batch_index=1550, loss=0.20958495140075684\n",
            "batch_index=1560, loss=0.49759775400161743\n",
            "batch_index=1570, loss=0.24657151103019714\n",
            "batch_index=1580, loss=0.04388187825679779\n",
            "batch_index=1590, loss=0.1307525634765625\n",
            "batch_index=1600, loss=0.2991507649421692\n",
            "batch_index=1610, loss=0.33542123436927795\n",
            "batch_index=1620, loss=0.08320087194442749\n",
            "batch_index=1630, loss=0.07032377272844315\n",
            "batch_index=1640, loss=0.21107664704322815\n",
            "batch_index=1650, loss=0.20074909925460815\n",
            "batch_index=1660, loss=0.425191193819046\n",
            "batch_index=1670, loss=0.20370852947235107\n",
            "batch_index=1680, loss=0.07657241076231003\n",
            "batch_index=1690, loss=0.530961275100708\n",
            "batch_index=1700, loss=0.1309252232313156\n",
            "batch_index=1710, loss=0.23307691514492035\n",
            "batch_index=1720, loss=0.1630350947380066\n",
            "batch_index=1730, loss=0.04328543692827225\n",
            "batch_index=1740, loss=0.4746687710285187\n",
            "batch_index=1750, loss=0.21337035298347473\n",
            "batch_index=1760, loss=0.07395131886005402\n",
            "batch_index=1770, loss=0.04997459053993225\n",
            "batch_index=1780, loss=0.3177661895751953\n",
            "batch_index=1790, loss=0.2859552800655365\n",
            "batch_index=1800, loss=0.7320334911346436\n",
            "batch_index=1810, loss=0.3142887055873871\n",
            "batch_index=1820, loss=0.09392127394676208\n",
            "batch_index=1830, loss=0.10283707082271576\n",
            "batch_index=1840, loss=0.35789063572883606\n",
            "batch_index=1850, loss=0.22518354654312134\n",
            "batch_index=1860, loss=0.2077450305223465\n",
            "batch_index=1870, loss=0.5599550008773804\n",
            "batch_index=1880, loss=0.06521540880203247\n",
            "batch_index=1890, loss=0.3883904218673706\n",
            "batch_index=1900, loss=0.07648184895515442\n",
            "batch_index=1910, loss=0.509381890296936\n",
            "batch_index=1920, loss=0.08699019998311996\n",
            "batch_index=1930, loss=0.6276665925979614\n",
            "batch_index=1940, loss=0.05085614696145058\n",
            "batch_index=1950, loss=0.17951886355876923\n",
            "batch_index=1960, loss=0.0546349361538887\n",
            "batch_index=1970, loss=0.5373315811157227\n",
            "batch_index=1980, loss=0.050619155168533325\n",
            "batch_index=1990, loss=0.24361631274223328\n",
            "batch_index=2000, loss=0.18334323167800903\n",
            "batch_index=2010, loss=0.2478756159543991\n",
            "batch_index=2020, loss=0.14922183752059937\n",
            "batch_index=2030, loss=0.20668208599090576\n",
            "batch_index=2040, loss=0.26962295174598694\n",
            "batch_index=2050, loss=0.03958086669445038\n",
            "batch_index=2060, loss=0.17532894015312195\n",
            "batch_index=2070, loss=0.32467159628868103\n",
            "batch_index=2080, loss=0.043489474803209305\n",
            "batch_index=2090, loss=0.030851120129227638\n",
            "batch_index=2100, loss=0.23290801048278809\n",
            "batch_index=2110, loss=0.5609311461448669\n",
            "batch_index=2120, loss=0.06877356767654419\n",
            "batch_index=2130, loss=0.03558582440018654\n",
            "batch_index=2140, loss=0.14871804416179657\n",
            "batch_index=2150, loss=0.1791246086359024\n",
            "batch_index=2160, loss=0.3913195729255676\n",
            "batch_index=2170, loss=0.06390777230262756\n",
            "batch_index=2180, loss=0.07103647291660309\n",
            "batch_index=2190, loss=0.8515965342521667\n",
            "batch_index=2200, loss=0.18087998032569885\n",
            "batch_index=2210, loss=0.07850635051727295\n",
            "batch_index=2220, loss=0.36627355217933655\n",
            "batch_index=2230, loss=0.4469969868659973\n",
            "batch_index=2240, loss=0.08249867707490921\n",
            "batch_index=2250, loss=0.2622787654399872\n",
            "batch_index=2260, loss=0.159382164478302\n",
            "batch_index=2270, loss=0.04898594319820404\n",
            "batch_index=2280, loss=0.09305444359779358\n",
            "batch_index=2290, loss=0.5459816455841064\n",
            "batch_index=2300, loss=0.8443484306335449\n",
            "batch_index=2310, loss=0.33204442262649536\n",
            "batch_index=2320, loss=0.0523611381649971\n",
            "batch_index=2330, loss=0.43963056802749634\n",
            "batch_index=2340, loss=0.10240734368562698\n",
            "batch_index=2350, loss=0.021117713302373886\n",
            "batch_index=2360, loss=0.30923059582710266\n",
            "batch_index=2370, loss=0.09522534906864166\n",
            "batch_index=2380, loss=0.3420487344264984\n",
            "batch_index=2390, loss=0.29346367716789246\n",
            "batch_index=2400, loss=0.10005941241979599\n",
            "batch_index=2410, loss=0.29365965723991394\n",
            "batch_index=2420, loss=0.19478636980056763\n",
            "batch_index=2430, loss=0.06526833772659302\n",
            "batch_index=2440, loss=0.2187158167362213\n",
            "batch_index=2450, loss=0.6825546622276306\n",
            "batch_index=2460, loss=0.04277285560965538\n",
            "batch_index=2470, loss=0.3557751774787903\n",
            "batch_index=2480, loss=0.05967900902032852\n",
            "batch_index=2490, loss=0.6424354910850525\n",
            "batch_index=2500, loss=0.41546398401260376\n",
            "batch_index=2510, loss=0.2738416790962219\n",
            "batch_index=2520, loss=0.20415756106376648\n",
            "batch_index=2530, loss=0.13575458526611328\n",
            "batch_index=2540, loss=0.18363261222839355\n",
            "batch_index=2550, loss=0.2957935929298401\n",
            "batch_index=2560, loss=0.14696621894836426\n",
            "batch_index=2570, loss=0.27526164054870605\n",
            "batch_index=2580, loss=0.9760048389434814\n",
            "batch_index=2590, loss=0.23571830987930298\n",
            "batch_index=2600, loss=0.047803908586502075\n",
            "batch_index=2610, loss=0.06629288196563721\n",
            "batch_index=2620, loss=0.7964973449707031\n",
            "batch_index=2630, loss=0.215843066573143\n",
            "batch_index=2640, loss=0.7260205745697021\n",
            "batch_index=2650, loss=0.2884509861469269\n",
            "batch_index=2660, loss=0.12314858287572861\n",
            "batch_index=2670, loss=0.13905251026153564\n",
            "batch_index=2680, loss=0.05824233591556549\n",
            "batch_index=2690, loss=0.2221289575099945\n",
            "batch_index=2700, loss=0.13168083131313324\n",
            "batch_index=2710, loss=0.3545897305011749\n",
            "batch_index=2720, loss=0.18835927546024323\n",
            "batch_index=2730, loss=0.07997296750545502\n",
            "batch_index=2740, loss=0.17468132078647614\n",
            "batch_index=2750, loss=0.17087562382221222\n",
            "batch_index=2760, loss=0.2995472550392151\n",
            "batch_index=2770, loss=0.0605182945728302\n",
            "batch_index=2780, loss=0.320528507232666\n",
            "batch_index=2790, loss=0.4432116448879242\n",
            "batch_index=2800, loss=0.1337079107761383\n",
            "batch_index=2810, loss=0.03821702301502228\n",
            "batch_index=2820, loss=0.04178629070520401\n",
            "batch_index=2830, loss=0.01966938003897667\n",
            "batch_index=2840, loss=0.23558853566646576\n",
            "batch_index=2850, loss=0.5137482285499573\n",
            "batch_index=2860, loss=0.043260253965854645\n",
            "batch_index=2870, loss=0.07781679928302765\n",
            "batch_index=2880, loss=0.460991770029068\n",
            "batch_index=2890, loss=0.2287347912788391\n",
            "batch_index=2900, loss=0.38228029012680054\n",
            "batch_index=2910, loss=0.2299806773662567\n",
            "batch_index=2920, loss=0.28503724932670593\n",
            "batch_index=2930, loss=0.3003126382827759\n",
            "batch_index=2940, loss=0.05795707181096077\n",
            "batch_index=2950, loss=0.012182086706161499\n",
            "batch_index=2960, loss=0.09101414680480957\n",
            "batch_index=2970, loss=0.07410865277051926\n",
            "batch_index=2980, loss=0.5356961488723755\n",
            "batch_index=2990, loss=0.08954650908708572\n",
            "batch_index=3000, loss=0.429405152797699\n",
            "batch_index=3010, loss=0.18696241080760956\n",
            "batch_index=3020, loss=0.42663633823394775\n",
            "batch_index=3030, loss=0.15468814969062805\n",
            "batch_index=3040, loss=0.012952187098562717\n",
            "batch_index=3050, loss=0.3139023184776306\n",
            "batch_index=3060, loss=0.12022767961025238\n",
            "batch_index=3070, loss=0.48243895173072815\n",
            "batch_index=3080, loss=0.035667017102241516\n",
            "batch_index=3090, loss=0.03009773977100849\n",
            "batch_index=3100, loss=0.2402939796447754\n",
            "batch_index=3110, loss=0.7183822393417358\n",
            "batch_index=3120, loss=0.29522138833999634\n",
            "batch_index=3130, loss=0.300917387008667\n",
            "batch_index=3140, loss=0.251895934343338\n",
            "batch_index=3150, loss=0.8322713375091553\n",
            "batch_index=3160, loss=0.42698755860328674\n",
            "batch_index=3170, loss=0.16465961933135986\n",
            "batch_index=3180, loss=0.6162460446357727\n",
            "batch_index=3190, loss=0.12763947248458862\n",
            "batch_index=3200, loss=0.18788692355155945\n",
            "batch_index=3210, loss=0.09188812971115112\n",
            "batch_index=3220, loss=0.2023116499185562\n",
            "batch_index=3230, loss=0.011041363701224327\n",
            "batch_index=3240, loss=0.2597370445728302\n",
            "batch_index=3250, loss=0.2316291630268097\n",
            "batch_index=3260, loss=0.07931242883205414\n",
            "batch_index=3270, loss=0.11482959240674973\n",
            "batch_index=3280, loss=0.04296959564089775\n",
            "batch_index=3290, loss=0.252307265996933\n",
            "batch_index=3300, loss=0.43826061487197876\n",
            "batch_index=3310, loss=0.2132120579481125\n",
            "batch_index=3320, loss=0.3054846525192261\n",
            "batch_index=3330, loss=0.12721183896064758\n",
            "batch_index=3340, loss=0.06285020709037781\n",
            "batch_index=3350, loss=0.2020539939403534\n",
            "batch_index=3360, loss=0.5429909825325012\n",
            "batch_index=3370, loss=0.23885545134544373\n",
            "batch_index=3380, loss=0.17267361283302307\n",
            "batch_index=3390, loss=0.06650635600090027\n",
            "batch_index=3400, loss=0.09838595986366272\n",
            "batch_index=3410, loss=0.06150571256875992\n",
            "batch_index=3420, loss=0.3080732822418213\n",
            "batch_index=3430, loss=0.10921531170606613\n",
            "batch_index=3440, loss=0.4439380168914795\n",
            "batch_index=3450, loss=0.4281306564807892\n",
            "batch_index=3460, loss=0.03974638879299164\n",
            "batch_index=3470, loss=0.1525273323059082\n",
            "batch_index=3480, loss=0.2912787199020386\n",
            "batch_index=3490, loss=0.23952457308769226\n",
            "batch_index=3500, loss=0.10488991439342499\n",
            "batch_index=3510, loss=0.08816066384315491\n",
            "batch_index=3520, loss=0.10477626323699951\n",
            "batch_index=3530, loss=0.048754237592220306\n",
            "batch_index=3540, loss=0.06665971875190735\n",
            "batch_index=3550, loss=0.120761938393116\n",
            "batch_index=3560, loss=0.09978149831295013\n",
            "batch_index=3570, loss=0.10762590169906616\n",
            "batch_index=3580, loss=0.1475682556629181\n",
            "batch_index=3590, loss=0.05618042126297951\n",
            "batch_index=3600, loss=0.036123231053352356\n",
            "batch_index=3610, loss=0.19804073870182037\n",
            "batch_index=3620, loss=0.23729217052459717\n",
            "batch_index=3630, loss=0.04618556424975395\n",
            "batch_index=3640, loss=0.028378697112202644\n",
            "batch_index=3650, loss=0.24832600355148315\n",
            "batch_index=3660, loss=0.22333315014839172\n",
            "batch_index=3670, loss=0.07116567343473434\n",
            "batch_index=3680, loss=0.12053290754556656\n",
            "batch_index=3690, loss=0.32193723320961\n",
            "batch_index=3700, loss=0.04367654398083687\n",
            "batch_index=3710, loss=0.03884459286928177\n",
            "batch_index=3720, loss=0.4895225167274475\n",
            "batch_index=3730, loss=0.008295439183712006\n",
            "batch_index=3740, loss=0.22834695875644684\n",
            "batch_index=3750, loss=0.5538519024848938\n",
            "batch_index=3760, loss=0.2545875310897827\n",
            "batch_index=3770, loss=0.11404389142990112\n",
            "batch_index=3780, loss=0.08797992765903473\n",
            "batch_index=3790, loss=0.5503684878349304\n",
            "batch_index=3800, loss=0.6108020544052124\n",
            "batch_index=3810, loss=0.1372060924768448\n",
            "batch_index=3820, loss=0.09991803765296936\n",
            "batch_index=3830, loss=0.15131333470344543\n",
            "batch_index=3840, loss=0.5664801001548767\n",
            "batch_index=3850, loss=0.08130530267953873\n",
            "batch_index=3860, loss=0.1624966561794281\n",
            "batch_index=3870, loss=0.19192644953727722\n",
            "batch_index=3880, loss=0.1642485409975052\n",
            "batch_index=3890, loss=0.07215180993080139\n",
            "batch_index=3900, loss=0.20284327864646912\n",
            "batch_index=3910, loss=0.13380634784698486\n",
            "batch_index=3920, loss=0.15406665205955505\n",
            "batch_index=3930, loss=0.21510839462280273\n",
            "batch_index=3940, loss=0.2361610233783722\n",
            "batch_index=3950, loss=0.10423470288515091\n",
            "batch_index=3960, loss=0.06541316211223602\n",
            "batch_index=3970, loss=0.43229684233665466\n",
            "batch_index=3980, loss=0.17497488856315613\n",
            "batch_index=3990, loss=0.2522366940975189\n",
            "batch_index=4000, loss=0.18200384080410004\n",
            "batch_index=4010, loss=0.28264257311820984\n",
            "batch_index=4020, loss=0.14590853452682495\n",
            "batch_index=4030, loss=0.18765607476234436\n",
            "batch_index=4040, loss=0.68721604347229\n",
            "batch_index=4050, loss=0.02317778952419758\n",
            "batch_index=4060, loss=0.4890134632587433\n",
            "batch_index=4070, loss=0.2591981291770935\n",
            "batch_index=4080, loss=0.5009121298789978\n",
            "batch_index=4090, loss=0.24281847476959229\n",
            "batch_index=4100, loss=0.9469987750053406\n",
            "batch_index=4110, loss=0.03122583031654358\n",
            "batch_index=4120, loss=0.03803247958421707\n",
            "batch_index=4130, loss=0.007863394916057587\n",
            "batch_index=4140, loss=0.0590335875749588\n",
            "batch_index=4150, loss=0.5497990250587463\n",
            "batch_index=4160, loss=0.07041831314563751\n",
            "batch_index=4170, loss=0.09767665714025497\n",
            "batch_index=4180, loss=0.037503570318222046\n",
            "batch_index=4190, loss=0.5894796848297119\n",
            "batch_index=4200, loss=0.06648226827383041\n",
            "batch_index=4210, loss=0.2758045792579651\n",
            "batch_index=4220, loss=0.7375887036323547\n",
            "batch_index=4230, loss=0.22489354014396667\n",
            "batch_index=4240, loss=0.2911207675933838\n",
            "batch_index=4250, loss=0.5068977475166321\n",
            "batch_index=4260, loss=0.3491780161857605\n",
            "batch_index=4270, loss=0.036399900913238525\n",
            "batch_index=4280, loss=0.1112058237195015\n",
            "batch_index=4290, loss=0.026890790089964867\n",
            "batch_index=4300, loss=0.04123695194721222\n",
            "batch_index=4310, loss=0.8130896687507629\n",
            "batch_index=4320, loss=0.10265274345874786\n",
            "batch_index=4330, loss=0.13097821176052094\n",
            "batch_index=4340, loss=0.044433966279029846\n",
            "batch_index=4350, loss=0.10202524065971375\n",
            "batch_index=4360, loss=0.045196227729320526\n",
            "batch_index=4370, loss=0.29060447216033936\n",
            "batch_index=4380, loss=0.02408858947455883\n",
            "batch_index=4390, loss=0.34724026918411255\n",
            "batch_index=4400, loss=0.23488283157348633\n",
            "batch_index=4410, loss=0.06737373769283295\n",
            "batch_index=4420, loss=0.057515949010849\n",
            "batch_index=4430, loss=0.026832781732082367\n",
            "batch_index=4440, loss=0.32595449686050415\n",
            "batch_index=4450, loss=0.2582958936691284\n",
            "batch_index=4460, loss=0.59288489818573\n",
            "batch_index=4470, loss=0.2928050458431244\n",
            "batch_index=4480, loss=0.0522458553314209\n",
            "batch_index=4490, loss=0.28459659218788147\n",
            "batch_index=4500, loss=0.1959795504808426\n",
            "batch_index=4510, loss=0.15789014101028442\n",
            "batch_index=4520, loss=0.015508748590946198\n",
            "batch_index=4530, loss=0.04495668783783913\n",
            "batch_index=4540, loss=0.02400318905711174\n",
            "batch_index=4550, loss=0.1301461011171341\n",
            "batch_index=4560, loss=0.03453747183084488\n",
            "batch_index=4570, loss=0.028724826872348785\n",
            "batch_index=4580, loss=0.10586279630661011\n",
            "batch_index=4590, loss=0.033722762018442154\n",
            "batch_index=4600, loss=0.03381852060556412\n",
            "batch_index=4610, loss=0.11355521529912949\n",
            "batch_index=4620, loss=0.3076098561286926\n",
            "batch_index=4630, loss=0.40294548869132996\n",
            "batch_index=4640, loss=0.2610012888908386\n",
            "batch_index=4650, loss=0.05401545763015747\n",
            "batch_index=4660, loss=0.07399144768714905\n",
            "batch_index=4670, loss=0.015906991437077522\n",
            "batch_index=4680, loss=0.0633666142821312\n",
            "batch_index=4690, loss=0.09062443673610687\n",
            "batch_index=4700, loss=0.24177943170070648\n",
            "batch_index=4710, loss=0.7534531950950623\n",
            "batch_index=4720, loss=0.6490955948829651\n",
            "batch_index=4730, loss=0.10528695583343506\n",
            "batch_index=4740, loss=0.06474748253822327\n",
            "batch_index=4750, loss=0.22230061888694763\n",
            "batch_index=4760, loss=0.5621564388275146\n",
            "batch_index=4770, loss=0.16835036873817444\n",
            "batch_index=4780, loss=0.14424839615821838\n",
            "batch_index=4790, loss=0.045177560299634933\n",
            "batch_index=4800, loss=0.06705957651138306\n",
            "batch_index=4810, loss=0.1909598708152771\n",
            "batch_index=4820, loss=0.7408252954483032\n",
            "batch_index=4830, loss=0.06573711335659027\n",
            "batch_index=4840, loss=0.06693302094936371\n",
            "batch_index=4850, loss=0.48056089878082275\n",
            "batch_index=4860, loss=0.02583940699696541\n",
            "batch_index=4870, loss=0.43610918521881104\n",
            "batch_index=4880, loss=0.13687612116336823\n",
            "batch_index=4890, loss=0.10544043779373169\n",
            "batch_index=4900, loss=0.029833614826202393\n",
            "batch_index=4910, loss=0.14738084375858307\n",
            "batch_index=4920, loss=0.39660245180130005\n",
            "batch_index=4930, loss=0.38591641187667847\n",
            "batch_index=4940, loss=0.07830115407705307\n",
            "batch_index=4950, loss=0.33955058455467224\n",
            "batch_index=4960, loss=0.20046821236610413\n",
            "batch_index=4970, loss=0.02713177725672722\n",
            "batch_index=4980, loss=0.39901432394981384\n",
            "batch_index=4990, loss=0.015294410288333893\n",
            "batch_index=5000, loss=0.30753546953201294\n",
            "batch_index=5010, loss=0.1715114563703537\n",
            "batch_index=5020, loss=0.1954946219921112\n",
            "batch_index=5030, loss=0.0815032571554184\n",
            "batch_index=5040, loss=0.5675279498100281\n",
            "batch_index=5050, loss=0.2259427160024643\n",
            "batch_index=5060, loss=0.05129121243953705\n",
            "batch_index=5070, loss=0.09968357533216476\n",
            "batch_index=5080, loss=0.3201274573802948\n",
            "batch_index=5090, loss=0.1936562955379486\n",
            "batch_index=5100, loss=0.2054932415485382\n",
            "batch_index=5110, loss=0.03657470643520355\n",
            "batch_index=5120, loss=0.222274512052536\n",
            "batch_index=5130, loss=0.0878397673368454\n",
            "batch_index=5140, loss=0.28329628705978394\n",
            "batch_index=5150, loss=0.10953420400619507\n",
            "batch_index=5160, loss=0.19763636589050293\n",
            "batch_index=5170, loss=0.06972311437129974\n",
            "batch_index=5180, loss=0.4182214140892029\n",
            "batch_index=5190, loss=0.3697647154331207\n",
            "batch_index=5200, loss=0.2595174312591553\n",
            "batch_index=5210, loss=0.18974533677101135\n",
            "batch_index=5220, loss=0.19980446994304657\n",
            "batch_index=5230, loss=0.8567711114883423\n",
            "batch_index=5240, loss=0.16087423264980316\n",
            "batch_index=5250, loss=0.2988970875740051\n",
            "batch_index=5260, loss=0.8109731674194336\n",
            "batch_index=5270, loss=0.4914795756340027\n",
            "batch_index=5280, loss=0.565345287322998\n",
            "batch_index=5290, loss=0.04665433242917061\n",
            "batch_index=5300, loss=0.2974777817726135\n",
            "batch_index=5310, loss=0.38570889830589294\n",
            "batch_index=5320, loss=0.3293149173259735\n",
            "batch_index=5330, loss=0.1851806342601776\n",
            "batch_index=5340, loss=0.05558177828788757\n",
            "batch_index=5350, loss=0.09013724327087402\n",
            "batch_index=5360, loss=0.060444191098213196\n",
            "batch_index=5370, loss=0.11000531166791916\n",
            "batch_index=5380, loss=0.2976300120353699\n",
            "batch_index=5390, loss=0.08076386898756027\n",
            "batch_index=5400, loss=0.10903257876634598\n",
            "batch_index=5410, loss=0.027944840490818024\n",
            "batch_index=5420, loss=0.24659235775470734\n",
            "batch_index=5430, loss=0.40912923216819763\n",
            "batch_index=5440, loss=0.12772870063781738\n",
            "batch_index=5450, loss=0.2856906056404114\n",
            "batch_index=5460, loss=0.28754186630249023\n",
            "batch_index=5470, loss=0.07664671540260315\n",
            "batch_index=5480, loss=0.22417859733104706\n",
            "batch_index=5490, loss=0.22413206100463867\n",
            "batch_index=5500, loss=0.20764654874801636\n",
            "batch_index=5510, loss=0.09226590394973755\n",
            "batch_index=5520, loss=0.5171190500259399\n",
            "batch_index=5530, loss=0.8959925174713135\n",
            "batch_index=5540, loss=0.30540719628334045\n",
            "batch_index=5550, loss=0.07439015805721283\n",
            "batch_index=5560, loss=0.2534511387348175\n",
            "batch_index=5570, loss=0.08523193001747131\n",
            "batch_index=5580, loss=0.48717477917671204\n",
            "batch_index=5590, loss=0.04485089331865311\n",
            "batch_index=5600, loss=0.038487114012241364\n",
            "batch_index=5610, loss=0.21455389261245728\n",
            "batch_index=5620, loss=0.20769894123077393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [1:06:17<9:56:41, 3977.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy score = 0.914\n",
            "XXXXXXXXXXXXXXXXXXXX EPOCH : 1 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "batch_index=0, loss=0.21776549518108368\n",
            "batch_index=10, loss=0.07216081768274307\n",
            "batch_index=20, loss=0.10269412398338318\n",
            "batch_index=30, loss=0.014318596571683884\n",
            "batch_index=40, loss=0.0632098838686943\n",
            "batch_index=50, loss=0.03609033674001694\n",
            "batch_index=60, loss=0.23828370869159698\n",
            "batch_index=70, loss=0.0776645764708519\n",
            "batch_index=80, loss=0.08358469605445862\n",
            "batch_index=90, loss=0.17218410968780518\n",
            "batch_index=100, loss=0.25186312198638916\n",
            "batch_index=110, loss=0.06747061014175415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5c63b34d97da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-5c63b34d97da>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"EPOCH :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d5479069e54f>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# find loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}